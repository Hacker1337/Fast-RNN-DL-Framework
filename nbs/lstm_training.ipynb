{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lets_plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_scalar(x, i):\n",
    "    \"\"\"Returns i-th element from an array x if x is array, else returns x\n",
    "Args:\n",
    "    x - input array\n",
    "    i - index to return\n",
    "Returns:\n",
    "    x[i] if x is an array and x else\n",
    "\"\"\"\n",
    "    if isinstance(x, (np.ndarray, list)):\n",
    "        return x[i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checker(J, grad_J, theta, eps=1e-4, rtol=1e-5):\n",
    "    \"\"\"Gradient checker for scalar and vector functions\n",
    "Args:\n",
    "    J - function of theta\n",
    "    grad_J - gradient of function J\n",
    "    theta - the point for which to compute the numerical gradient\n",
    "    eps - step value in numerical gradient\n",
    "    rtol - relative tolerance threshold value\n",
    "Returns:\n",
    "    error message if the relative tolerance is greater for some axis\n",
    "    or \"Gradient check passed\" else\n",
    "\"\"\"\n",
    "    it = np.nditer(theta, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        theta_ = np.array(theta, copy=True)\n",
    "        theta_[ix] += eps\n",
    "        np.random.seed(42)\n",
    "        J1 = J(theta_)\n",
    "\n",
    "        theta_ = np.array(theta, copy=True)\n",
    "        theta_[ix] -= eps\n",
    "        np.random.seed(42)\n",
    "        J2 = J(theta_)\n",
    "\n",
    "        J1 = to_scalar(J1, ix)\n",
    "        J2 = to_scalar(J2, ix)\n",
    "\n",
    "        num_grad = (J1 - J2)/(2*eps)\n",
    "\n",
    "        rel_tol = np.abs(num_grad - grad_J(theta))[ix]/(1. + np.minimum(np.abs(num_grad), np.abs(grad_J(theta)[ix])))\n",
    "\n",
    "        if np.all(rel_tol > rtol):\n",
    "            print(f'Incorrect gradient for the axis {str(ix)}')\n",
    "            return\n",
    "        it.iternext()\n",
    "    print(f'Gradient check passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_theta_global(model, loss_function, theta, idx, x, y):\n",
    "    previous = model.parameters()[idx].copy()\n",
    "    np.copyto(dst=model.parameters()[idx], src=theta)\n",
    "    outputs = model(x)\n",
    "    loss = loss_function(outputs, y)\n",
    "    np.copyto(dst=model.parameters()[idx], src=previous)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_theta_global(model, loss_function, theta, idx, x):\n",
    "    grad = model.backward(loss_function)[idx] / x.shape[0]\n",
    "    return grad.reshape(theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,slope=1.0):\n",
    "    return 1.0/(1.0+np.exp(-slope*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x,slope=1.0):\n",
    "    return slope*sigmoid(x,slope=slope)*(1.0-sigmoid(x,slope=slope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed\n"
     ]
    }
   ],
   "source": [
    "z = np.random.normal(size=5)\n",
    "gradient_checker(sigmoid, sigmoid_prime, z, eps=1e-4, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gg_confusion_matrix(y, y_hat):\n",
    "    conf_mat = confusion_matrix(y, y_hat)[::-1]\n",
    "    confusion_dat = pd.DataFrame(conf_mat)\n",
    "    observed = confusion_dat.columns.values\n",
    "    actual = confusion_dat.index.values\n",
    "    xx, yy = np.meshgrid(actual, observed)\n",
    "    xx = xx.reshape(-1)\n",
    "    yy = yy.reshape(-1)\n",
    "    zz = conf_mat.reshape(-1)\n",
    "    dat = {'predicted':xx, 'actual':yy[::-1], 'z':zz}\n",
    "    p = ggplot(dat, aes('predicted', 'actual', fill='z')) \\\n",
    "        + geom_raster() \\\n",
    "        + geom_text(aes(label='z'), color='white')\\\n",
    "        + theme(legend_position='none', axis_ticks='blank', axis_line='blank')\\\n",
    "        + ggsize(500, 500) + scale_x_discrete() + scale_y_discrete()\\\n",
    "        + ggtitle('Confusion matrix')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import inspect\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union, Callable\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 1000\n",
    "seq_len = 3\n",
    "max_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(seq_len, num_examples, max_number):\n",
    "    inputs = np.random.randint(0, max_number, size=(num_examples, seq_len))\n",
    "    targets = np.sort(inputs)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = get_examples(seq_len, num_examples, max_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs/sorted: ([6, 3, 7], [3, 6, 7])\n",
      "inputs/sorted: ([4, 6, 9], [4, 6, 9])\n",
      "inputs/sorted: ([2, 6, 7], [2, 6, 7])\n",
      "inputs/sorted: ([4, 3, 7], [3, 4, 7])\n",
      "inputs/sorted: ([7, 2, 5], [2, 5, 7])\n",
      "inputs/sorted: ([4, 1, 7], [1, 4, 7])\n",
      "inputs/sorted: ([5, 1, 4], [1, 4, 5])\n",
      "inputs/sorted: ([0, 9, 5], [0, 5, 9])\n",
      "inputs/sorted: ([8, 0, 9], [0, 8, 9])\n",
      "inputs/sorted: ([2, 6, 3], [2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "for inp, tgt in zip(inputs[:10], targets[:10]):\n",
    "    print(f'inputs/sorted: {list(inp), list(tgt)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(X, seq_len, max_number):\n",
    "    max_length = int(seq_len * np.ceil(np.log10(max_number + 1)) + seq_len - 1)\n",
    "    Xstr = []\n",
    "    for example in X:\n",
    "        xstr = ','.join([str(n) for n in example])\n",
    "        xstr += ''.join([' ' for _ in range(max_length - len(xstr))])\n",
    "        Xstr.append(xstr)\n",
    "    return Xstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = to_string(inputs, seq_len, max_number)\n",
    "targets = to_string(targets, seq_len, max_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[0]: 6,3,7    targets[0]: 3,6,7   \n"
     ]
    }
   ],
   "source": [
    "print(f'inputs[0]: {inputs[0]} targets[0]: {targets[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ',', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encode(X, vocab):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(vocab))\n",
    "    Xenc = []\n",
    "    for example in X:\n",
    "        encoded = [char_to_int[char] for char in example]\n",
    "        Xenc.append(encoded)\n",
    "    return Xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = integer_encode(inputs, vocab)\n",
    "targets = integer_encode(targets, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[0]: [6, 10, 3, 10, 7, 11, 11, 11] targets[0]: [3, 10, 6, 10, 7, 11, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "print(f'inputs[0]: {inputs[0]} targets[0]: {targets[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inputs = np.array(inputs)\n",
    "dataset_targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_deep_rnn.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Module):\n",
    "    def __init__(self, state_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.linear = LinearLayer(state_size, hidden_size)\n",
    "        self.tanh = TanhFunction()\n",
    "        self.hstack = HStack()\n",
    "        self.register_parameters([self.linear])\n",
    "\n",
    "    def forward(self, x: Tensor, h_t_1: Optional[Tensor] = None):\n",
    "        X = self.hstack(x, h_t_1)\n",
    "        z = self.linear(X)\n",
    "        h_t = self.tanh(z)\n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = input_size + hidden_size\n",
    "        self.rnn = RNNCell(self.state_size, hidden_size)\n",
    "        self. row = Row()\n",
    "        self.vstack = VStack()\n",
    "        self.register_parameters([self.rnn])\n",
    "\n",
    "    def forward(self, x: Tensor, h_t_1: Optional[Tensor] = None):\n",
    "        seq_len, batch_size, input_size = x.shape\n",
    "        # print(f'seq_len: {seq_len} batch_size: {batch_size} input_size: {input_size}')\n",
    "        h = Tensor(np.zeros((0, batch_size, self.hidden_size)), name=\"h\")\n",
    "        if h_t_1 is None:\n",
    "            h_t_1 = Tensor(np.zeros((batch_size, self.hidden_size)), name=\"h_t_1\")\n",
    "        for idx in range(seq_len):\n",
    "            h_t_1 = self.rnn.forward(self.row(x, idx), h_t_1)\n",
    "            h = self.vstack(h, h_t_1.reshape((1, batch_size, self.hidden_size)))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNetwork(Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(vocab_size, emb_size)\n",
    "        self.rnn = RNN(emb_size, hidden_size)\n",
    "        self.linear = LinearLayer(hidden_size, vocab_size)\n",
    "        xavier_(self.linear.parameters)\n",
    "        self.register_parameters([self.embedding, self.rnn, self.linear])\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        emb = self.embedding(x)\n",
    "        rnn_out = self.rnn(emb)\n",
    "        linear_out = self.linear(rnn_out.reshape(-1, self.hidden_size))\n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(model, val, y_val):\n",
    "    model.eval()\n",
    "    output = model(Tensor(val))\n",
    "    y_hat = np.argmax(output.data, axis=1)\n",
    "    model.train()\n",
    "    return accuracy_score(y_val.ravel(), y_hat.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "vocab_size = len(vocab)\n",
    "emb_size = 20\n",
    "hidden_size = 32\n",
    "batch_size = 100\n",
    "dataloader = DataLoader(dataset_inputs, dataset_targets, batch_size=batch_size)\n",
    "model = RecurrentNetwork(vocab_size, emb_size, hidden_size)\n",
    "loss_function = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters, lr=1.0)\n",
    "# optimizer = Adam(model.parameters, alpha=0.1, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01)\n",
    "scheduler = ConstantLR(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2332"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: [1/1], loss: 16.662478383112063, acc: 0.675"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "lrs = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss_sum = 0\n",
    "    for data in dataloader():\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        targets = targets.transpose(1, 0)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.data\n",
    "    acc = eval_accuracy(model, inputs.data, targets.data)\n",
    "    print(f'\\r epoch: [{epoch+1}/{num_epochs}], loss: {loss_sum}, acc: {acc}', end='')\n",
    "    losses.append(loss_sum)\n",
    "    accuracies.append(acc)\n",
    "    lrs.append(scheduler.lr)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(num_epochs)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].plot(epochs, losses)\n",
    "ax[0].set_title('Losses')\n",
    "ax[1].plot(epochs, accuracies)\n",
    "ax[1].set_title('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_examples = 100\n",
    "# seq_len = 3\n",
    "# max_number = 10\n",
    "\n",
    "# inputs, targets = get_examples(seq_len, num_examples, max_number)\n",
    "\n",
    "# inputs = to_string(inputs, seq_len, max_number)\n",
    "# targets = to_string(targets, seq_len, max_number)\n",
    "\n",
    "# inputs = integer_encode(inputs, vocab)\n",
    "# targets = integer_encode(targets, vocab)\n",
    "\n",
    "# inputs, targets = Tensor(np.array(inputs).transpose((1, 0))), Tensor(np.array(targets).transpose((1, 0)))\n",
    "# outputs = model(inputs)\n",
    "# predicted = np.argmax(outputs.data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gg_confusion_matrix(targets.data.reshape(-1), predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 100\n",
    "seq_len = 2\n",
    "max_number = 10\n",
    "vocab_size = len(vocab)\n",
    "emb_size = 20\n",
    "hidden_size = 32\n",
    "\n",
    "X_val, y_val = get_examples(seq_len, num_examples, max_number)\n",
    "X_val, y_val = X_val.transpose(1, 0), y_val.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: Start -- E\n",
      "Gradient check passed\n",
      "[0]: Elapsed time: 0.7s\n",
      "[1]: Start -- weights\n",
      "Gradient check passed\n",
      "[1]: Elapsed time: 3.9s\n",
      "[2]: Start -- bias\n",
      "Gradient check passed\n",
      "[2]: Elapsed time: 0.1s\n",
      "[3]: Start -- weights\n",
      "Gradient check passed\n",
      "[3]: Elapsed time: 0.7s\n",
      "[4]: Start -- bias\n",
      "Gradient check passed\n",
      "[4]: Elapsed time: 0.0s\n",
      "Total elapsed time: 5.4s\n"
     ]
    }
   ],
   "source": [
    "loss_function = CrossEntropyLoss()\n",
    "model_ = RecurrentNetwork(vocab_size, emb_size, hidden_size)\n",
    "dJ_theta_tensors = dJ_theta_global(model_, loss_function, Tensor(X_val), Tensor(y_val))\n",
    "global_start = time.time()\n",
    "for i, parameter in enumerate(model_.parameters):\n",
    "    start = time.time()\n",
    "    print(f'[{i}]: Start -- {parameter.__name__}')\n",
    "    def J_theta(theta, idx=i, x=Tensor(X_val), y=Tensor(y_val)):\n",
    "        return J_theta_global(model_, loss_function, theta, idx, x, y)\n",
    "    gradient_checker(J_theta, dJ_theta_tensors[i], parameter.data)\n",
    "    print(f'[{i}]: Elapsed time: {time.time() - start:.1f}s')\n",
    "print(f'Total elapsed time: {time.time() - global_start:.1f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (6.0)(OK)\n",
    "\n",
    "Implement the LSTM cell class and use it in the RNN sorter. Don't forget to check gradients!\n",
    "\n",
    "LSTM cell formulas:\n",
    "$$\n",
    "f_t=\\sigma\\Bigg(W_f\\cdot\\Big[h_{t-1};x_t\\Big] + b_f\\Bigg)\\\\\n",
    "i_t = \\sigma\\Bigg(W_i\\cdot\\Big[h_{t-1};x_t\\Big] + b_i\\Bigg)\\\\\n",
    "\\tilde{c}_t = \\tanh\\Bigg(W_c\\cdot\\Big[h_{t-1};x_t\\Big] + b_c\\Bigg)\\\\\n",
    "c_t = f_t \\ast c_{t-1} + i_t \\ast \\tilde{c}_t\\\\\n",
    "o_t = \\sigma\\Bigg(W_o\\cdot\\Big[h_{t-1};x_t\\Big] + b_o\\Bigg)\\\\\n",
    "h_t = o_t \\ast \\tanh \\left(c_t\\right)\n",
    "$$\n",
    "\n",
    "As you can see, you'll need two additional functions here: sum and multiplication. You have to implement them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Module):\n",
    "    def __init__(self, state_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.lin_f = LinearLayer(state_size, hidden_size)\n",
    "        self.lin_i = LinearLayer(state_size, hidden_size)\n",
    "        self.lin_c = LinearLayer(state_size, hidden_size)\n",
    "        self.lin_o = LinearLayer(state_size, hidden_size)\n",
    "        self.sigm = SigmoidFunction()\n",
    "        self.tanh = TanhFunction()\n",
    "        self.hstack = HStack()\n",
    "        self.sum = Sum()\n",
    "        self.prod = Multiply()\n",
    "        self.register_parameters([self.lin_f, self.lin_i, self.lin_c, self.lin_o])\n",
    "\n",
    "    def forward(self, x: Tensor, h_t_1: Tensor, c_t_1: Tensor):\n",
    "        h_x = self.hstack(h_t_1, x)\n",
    "        f_t = self.sigm(self.lin_f(h_x))\n",
    "        i_t = self.sigm(self.lin_i(h_x))\n",
    "        o_t = self.sigm(self.lin_o(h_x))\n",
    "\n",
    "        c_tild = self.tanh(self.lin_c(h_x))\n",
    "        c_t = self.sum(self.prod(f_t, c_t_1), self.prod(c_tild, i_t))\n",
    "        h_t = self.prod(o_t, self.tanh(c_t))\n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = input_size + hidden_size\n",
    "        self.rnn = LSTMCell(self.state_size, hidden_size)\n",
    "        self.row = Row()\n",
    "        self.vstack = VStack()\n",
    "        self.register_parameters([self.rnn])\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, h_t_1: Optional[Tensor] = None, c_t_1: Optional[Tensor] = None):\n",
    "        seq_len, batch_size, input_size = x.shape\n",
    "\n",
    "        h = Tensor(np.zeros((0, batch_size, self.hidden_size)), name=\"h\")\n",
    "        if h_t_1 is None:\n",
    "            h_t_1 = Tensor(np.zeros((batch_size, self.hidden_size)), name=\"h_t_1\")\n",
    "        if c_t_1 is None:\n",
    "            c_t_1 = Tensor(np.zeros((batch_size, self.hidden_size)), name=\"c_t_1\")\n",
    "        for idx in range(seq_len):\n",
    "            h_t_1, c_t_1 = self.rnn.forward(self.row(x, idx), h_t_1, c_t_1)\n",
    "            h = self.vstack(h, h_t_1.reshape((1, batch_size, self.hidden_size)))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71576708 0.8916259  1.74680369 0.94399938 1.30660592 1.8421169\n",
      "  0.41127958 1.58772063 0.49313897 0.32070423]\n",
      " [0.4560102  1.73079965 0.42882033 0.81700231 0.73318676 1.77478411\n",
      "  0.91791831 1.23505692 1.2962205  1.14340559]\n",
      " [1.33400104 1.73856441 0.16758543 0.88952964 0.74251496 0.80829794\n",
      "  0.9089686  0.90308372 0.57225288 1.49240337]\n",
      " [0.33105179 1.10656222 0.54507093 1.3863948  1.25762489 0.98710671\n",
      "  1.15801525 1.08883515 0.84523174 1.31342785]\n",
      " [1.2048898  0.74904879 0.60291771 1.27700176 0.97335936 0.78965347\n",
      "  0.27992526 1.15277419 0.3084457  0.33516707]\n",
      " [1.56583339 0.71093971 1.90755014 1.59153825 1.12172427 0.24694804\n",
      "  1.07982073 1.43250206 0.99272725 0.43338163]\n",
      " [1.31937761 0.41821415 0.92938775 1.69658672 0.40288297 0.93671595\n",
      "  0.93883652 1.95239634 0.61247384 1.07209153]\n",
      " [0.62393198 1.61562622 1.31617291 1.10039295 0.53855081 1.24747596\n",
      "  0.28205433 1.23178722 0.97834138 1.920653  ]\n",
      " [0.28902855 0.2181847  0.78791907 1.12085833 1.02313772 0.68508802\n",
      "  1.13421839 1.22465435 0.91056279 0.94951466]\n",
      " [1.16775226 0.96599293 1.36698715 1.0231469  0.74972171 1.18188682\n",
      "  1.13338623 0.67264327 0.779652   0.61758813]]\n",
      "[[0.12650388 0.16666897 0.75407298 0.12548614 0.40884263 0.84436624\n",
      "  0.007789   0.62122092 0.06075629 0.01134535]\n",
      " [0.02140175 0.7440423  0.00490669 0.13469438 0.11621883 0.78726843\n",
      "  0.07005042 0.37724571 0.41902293 0.31736176]\n",
      " [0.36908089 0.7535176  0.00527231 0.19712311 0.13711946 0.15450006\n",
      "  0.20654607 0.19935434 0.06049172 0.5024999 ]\n",
      " [0.0214863  0.16999606 0.03113577 0.47569862 0.354855   0.22588188\n",
      "  0.31317212 0.27819273 0.17631122 0.35901146]\n",
      " [0.31164441 0.06976531 0.05349882 0.3633178  0.0862958  0.10559275\n",
      "  0.01957563 0.32930459 0.00217483 0.00587942]\n",
      " [0.60178057 0.10982881 0.90908691 0.62711218 0.27384658 0.00916379\n",
      "  0.19804097 0.47499706 0.14122985 0.03907997]\n",
      " [0.32079227 0.04127778 0.14859442 0.71603257 0.03940591 0.02584667\n",
      "  0.19477376 0.95296282 0.0931572  0.1131306 ]\n",
      " [0.03054119 0.64285167 0.417208   0.29748606 0.07041334 0.38234837\n",
      "  0.01741038 0.37534941 0.1836272  0.92220485]\n",
      " [0.00745549 0.00368627 0.13220672 0.29265921 0.18395172 0.09519345\n",
      "  0.22837035 0.36499725 0.1917359  0.07658826]\n",
      " [0.32878581 0.13306131 0.45658044 0.19331641 0.13874085 0.29138675\n",
      "  0.23580653 0.06910578 0.12105203 0.06755406]]\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "a, b = Tensor(data=np.random.rand(10, 10)), Tensor(data=np.random.rand(10, 10))\n",
    "\n",
    "sum = SumFunction(a, b)()\n",
    "print(sum.data)\n",
    "sum.backward(sum.data)\n",
    "\n",
    "prod = MultiplyFunction(a, b)()\n",
    "print(prod.data)\n",
    "prod.backward(prod.data);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNetwork(Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(vocab_size, emb_size)\n",
    "        self.rnn = LSTM(emb_size, hidden_size)\n",
    "        self.linear = LinearLayer(hidden_size, vocab_size)\n",
    "        xavier_(self.linear.parameters)\n",
    "        self.register_parameters([self.embedding, self.rnn, self.linear])\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        emb = self.embedding(x)\n",
    "        rnn_out = self.rnn(emb)\n",
    "        linear_out = self.linear(rnn_out.reshape(-1, self.hidden_size))\n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "vocab_size = len(vocab)\n",
    "emb_size = 20\n",
    "hidden_size = 32\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset_inputs, dataset_targets, batch_size=batch_size)\n",
    "model = RecurrentNetwork(vocab_size, emb_size, hidden_size)\n",
    "loss_function = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters, lr=1.0)\n",
    "# optimizer = Adam(model.parameters, alpha=0.1, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01)\n",
    "scheduler = ConstantLR(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7420"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking training and evaluating time step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1)\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(dataloader()))\n",
    "inputs, targets = data\n",
    "inputs = inputs.transpose(1, 0)\n",
    "targets = targets.transpose(1, 0)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.73 ms ± 1.78 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# inference step\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.2 s ± 12.5 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# training step\n",
    "outputs = model(inputs)\n",
    "optimizer.zero_grad()\n",
    "loss = loss_function(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# accuracies = []\n",
    "# lrs = []\n",
    "# for epoch in range(num_epochs):\n",
    "#     loss_sum = 0\n",
    "#     for data in dataloader():\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs, targets = data\n",
    "#         inputs = inputs.transpose(1, 0)\n",
    "#         targets = targets.transpose(1, 0)\n",
    "#         outputs = model(inputs)\n",
    "#         loss = loss_function(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         loss_sum += loss.data\n",
    "#         print(\"done step\")\n",
    "#     acc = eval_accuracy(model, inputs.data, targets.data)\n",
    "#     print(f'\\r epoch: [{epoch+1}/{num_epochs}], loss: {loss_sum}, acc: {acc}', end='')\n",
    "#     losses.append(loss_sum)\n",
    "#     accuracies.append(acc)\n",
    "#     lrs.append(scheduler.lr)\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = np.arange(num_epochs)\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "# ax[0].plot(epochs, losses)\n",
    "# ax[0].set_title('Losses')\n",
    "# ax[1].plot(epochs, accuracies)\n",
    "# ax[1].set_title('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_examples = 100\n",
    "# seq_len = 3\n",
    "# max_number = 10\n",
    "\n",
    "# inputs, targets = get_examples(seq_len, num_examples, max_number)\n",
    "\n",
    "# inputs = to_string(inputs, seq_len, max_number)\n",
    "# targets = to_string(targets, seq_len, max_number)\n",
    "\n",
    "# inputs = integer_encode(inputs, vocab)\n",
    "# targets = integer_encode(targets, vocab)\n",
    "\n",
    "# inputs, targets = Tensor(np.array(inputs).transpose((1, 0))), Tensor(np.array(targets).transpose((1, 0)))\n",
    "# outputs = model(inputs)\n",
    "# predicted = np.argmax(outputs.data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gg_confusion_matrix(targets.data.reshape(-1), predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks\n",
    "Change RNN to LSTM in the RecurrentNetwork class below to perform gradient check.\n",
    "Note, that due to naive implementation of the backpropagation, number of calculations needed for one backpropagation pass grows exponentially with respect to sequence length. That's why the length in the gradient check is set to 2.\n",
    "\n",
    "\n",
    "$\\color{red}{\\text{If you want to implement a faster version of numpy framework, which supports recurrent arcgitectures, you can take this task as a course project.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNetwork(Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(vocab_size, emb_size)\n",
    "        self.lstm = LSTM(emb_size, hidden_size)\n",
    "        self.linear = LinearLayer(hidden_size, vocab_size)\n",
    "        xavier_(self.linear.parameters)\n",
    "        self.register_parameters([self.embedding, self.lstm, self.linear])\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        emb = self.embedding(x)\n",
    "        lstm_out = self.lstm(emb)\n",
    "        linear_out = self.linear(lstm_out.reshape(-1, self.hidden_size))\n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original - passes test\n",
    "num_examples = 100\n",
    "seq_len = 2\n",
    "max_number = 10\n",
    "vocab_size = len(vocab)\n",
    "emb_size = 20\n",
    "hidden_size = 32\n",
    "\n",
    "X_val, y_val = get_examples(seq_len, num_examples, max_number)\n",
    "X_val, y_val = X_val.transpose(1, 0), y_val.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: Start -- E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed. Calls: 240\n",
      "[0]: Elapsed time: 2.2s\n",
      "[1]: Start -- weights\n",
      "Gradient check passed. Calls: 1664\n",
      "[1]: Elapsed time: 8.3s\n",
      "[2]: Start -- bias\n",
      "Gradient check passed. Calls: 32\n",
      "[2]: Elapsed time: 0.1s\n",
      "[3]: Start -- weights\n",
      "Gradient check passed. Calls: 1664\n",
      "[3]: Elapsed time: 7.4s\n",
      "[4]: Start -- bias\n",
      "Gradient check passed. Calls: 32\n",
      "[4]: Elapsed time: 0.1s\n",
      "[5]: Start -- weights\n",
      "Gradient check passed. Calls: 1664\n",
      "[5]: Elapsed time: 7.8s\n",
      "[6]: Start -- bias\n",
      "Gradient check passed. Calls: 32\n",
      "[6]: Elapsed time: 0.1s\n",
      "[7]: Start -- weights\n",
      "Gradient check passed. Calls: 1664\n",
      "[7]: Elapsed time: 6.4s\n",
      "[8]: Start -- bias\n",
      "Gradient check passed. Calls: 32\n",
      "[8]: Elapsed time: 0.1s\n",
      "[9]: Start -- weights\n",
      "Gradient check passed. Calls: 384\n",
      "[9]: Elapsed time: 1.3s\n",
      "[10]: Start -- bias\n",
      "Gradient check passed. Calls: 12\n",
      "[10]: Elapsed time: 0.1s\n",
      "Total elapsed time: 33.9s\n"
     ]
    }
   ],
   "source": [
    "loss_function = CrossEntropyLoss()\n",
    "model_ = RecurrentNetwork(vocab_size, emb_size, hidden_size)\n",
    "dJ_theta_tensors = dJ_theta_global(model_, loss_function, Tensor(X_val), Tensor(y_val))\n",
    "global_start = time.time()\n",
    "for i, parameter in enumerate(model_.parameters):\n",
    "    start = time.time()\n",
    "    print(f'[{i}]: Start -- {parameter.__name__}')\n",
    "    def J_theta(theta, idx=i, x=Tensor(X_val), y=Tensor(y_val)):\n",
    "        return J_theta_global(model_, loss_function, theta, idx, x, y)\n",
    "    gradient_checker(J_theta, dJ_theta_tensors[i], parameter.data)\n",
    "    print(f'[{i}]: Elapsed time: {time.time() - start:.1f}s')\n",
    "print(f'Total elapsed time: {time.time() - global_start:.1f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
