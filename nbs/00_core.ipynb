{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lets_plot import *\n",
    "from typing import List, Optional, Union, Callable\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, grads, params, learning_rate=None):\n",
    "        if learning_rate is None:\n",
    "            learning_rate = self.learning_rate\n",
    "        for param, grad in zip(params, grads):\n",
    "            param -= learning_rate*grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def count_forward_calls(obj):\n",
    "    for prop, value in vars(obj).items():\n",
    "        if isinstance(value, Tensor):\n",
    "            value._forward_calls += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def forks_aware(FunctionClass):\n",
    "    class WrappedClass:\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            self.FunctionClass = FunctionClass(*args, **kwargs)\n",
    "\n",
    "        def __call__(self):\n",
    "            result = self.FunctionClass()\n",
    "            count_forward_calls(self.FunctionClass)\n",
    "            return result\n",
    "    return WrappedClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@forks_aware\n",
    "class Function:\n",
    "    def __call__(self) -> \"Tensor\":\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Tensor:\n",
    "    def __init__(self, data: np.ndarray, func: Optional[Function]=None, name: str=None):\n",
    "        self.data: np.ndarray = data\n",
    "        self.grad: np.ndarray = np.zeros(data.shape)\n",
    "        self.func = func\n",
    "        self.__name__ = name\n",
    "        self._forward_calls = 0\n",
    "        self._backward_calls = 0\n",
    "\n",
    "    def backward(self, grad: Optional[np.ndarray] = None):\n",
    "        self._backward_calls += 1\n",
    "        # print(f'{self.__name__} backward: {self._backward_calls} forward: {self._forward_calls}')\n",
    "        if grad is not None:\n",
    "            assert grad.shape == self.grad.shape\n",
    "            self.grad += grad\n",
    "            if self.func: # and self._forward_calls <= self._backward_calls:\n",
    "                self.func.backward(grad)\n",
    "        else:\n",
    "            if self.func:\n",
    "                self.func.backward()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad[:] = .0\n",
    "        self._forward_calls = 0\n",
    "        self._backward_calls = 0\n",
    "\n",
    "    def reshape(self, *args, **kwargs):\n",
    "        return Tensor(self.data.reshape(*args, **kwargs), self.func, self.__name__)\n",
    "\n",
    "    def transpose(self, *args, **kwargs):\n",
    "        return Tensor(self.data.transpose(*args, **kwargs), self.func, self.__name__)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.data.size\n",
    "\n",
    "    def astype(self, dtype: Union[str, np.dtype]):\n",
    "        return self.data.astype(dtype)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.parameters: List[Tensor] = []\n",
    "        self.__name__ = self.__class__.__name__\n",
    "        self.state_dict = {}\n",
    "        self.training = True\n",
    "\n",
    "    @staticmethod\n",
    "    def get_module_state_dict(module: \"Module\"):\n",
    "        keys = [param.__name__ for param in module.__dict__['parameters']]\n",
    "        values = [param.data.tolist() for param in module.__dict__['parameters']]\n",
    "        return dict(zip(keys, values))\n",
    "\n",
    "    def update_state_dict(self):\n",
    "        module_state_dicts = []\n",
    "        module_names = []\n",
    "        for key in self.__dict__:\n",
    "            value = self.__dict__[key]\n",
    "            base_class_name = value.__class__.__bases__[0].__name__\n",
    "            # class_name = value.__class__.__name__\n",
    "            if base_class_name == 'Module':\n",
    "                class_has_parameters = hasattr(value, \"parameters\")\n",
    "                if class_has_parameters:\n",
    "                    parameters_not_empty = len(value.parameters) > 0\n",
    "                    if parameters_not_empty:\n",
    "                        module_names.append(key)\n",
    "                        module_state_dict = self.get_module_state_dict(value)\n",
    "                        module_state_dicts.append(module_state_dict)\n",
    "        self.state_dict = dict(zip(module_names, module_state_dicts))\n",
    "\n",
    "    def register_parameter(self, param: Tensor):\n",
    "        self.parameters.append(param)\n",
    "\n",
    "    def register_parameters(self, param_list_or_module: Union[List[Tensor], \"Module\", List[\"Module\"]]):\n",
    "        if isinstance(param_list_or_module, List):\n",
    "            for element in param_list_or_module:\n",
    "                if isinstance(element, Tensor):\n",
    "                    self.register_parameter(element)\n",
    "                elif isinstance(element, Module):\n",
    "                    for param in element.parameters:\n",
    "                        self.register_parameter(param)\n",
    "                else:\n",
    "                    raise TypeError(f\"Parameter should be of type Tensor or Module, but got {element}\")\n",
    "        elif isinstance(param_list_or_module, Module):\n",
    "            for param in param_list_or_module.parameters:\n",
    "                self.register_parameter(param)\n",
    "        self.update_state_dict()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            param.zero_grad()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def size(self):\n",
    "        s = 0\n",
    "        for param in self.parameters:\n",
    "            s += param.data.size\n",
    "        return s\n",
    "\n",
    "    def update_parameters_from_state_dict(self):\n",
    "        for key in self.__dict__:\n",
    "            if key in self.state_dict:\n",
    "                for param in self.__dict__[key].parameters:\n",
    "                    param.data = np.asarray(self.state_dict[key][param.__name__])\n",
    "\n",
    "    def save(self, filename: str = None):\n",
    "        if filename is None:\n",
    "            filename = time.strftime(\"%Y%m%d-%H%M%S\") + '.json'\n",
    "        self.update_state_dict()\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.state_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def load(self, filename: str):\n",
    "        with open(filename, 'r') as f:\n",
    "            json_str = f.read()\n",
    "            self.state_dict = json.loads(json_str)\n",
    "        self.update_parameters_from_state_dict()\n",
    "\n",
    "    def train(self):\n",
    "        for key in self.__dict__:\n",
    "            module = self.__dict__[key]\n",
    "            base_class_name = module.__class__.__bases__[0].__name__\n",
    "            if base_class_name == 'Module':\n",
    "                module.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        for key in self.__dict__:\n",
    "            module = self.__dict__[key]\n",
    "            base_class_name = module.__class__.__bases__[0].__name__\n",
    "            if base_class_name == 'Module':\n",
    "                module.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def one_hot_encoder(inputs: Tensor, vocab_size: int):\n",
    "    seq_len, batch_size = inputs.shape\n",
    "    encoded = np.zeros((seq_len * batch_size, vocab_size))\n",
    "    encoded[np.arange(seq_len * batch_size), inputs.data.ravel().astype(int)] = 1\n",
    "    return Tensor(encoded.reshape(seq_len, batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def xavier_(weights):\n",
    "    for weight in weights:\n",
    "        in_dim, out_dim = weight.shape[-2:]\n",
    "        np.copyto(dst=weight.data, src=np.random.randn(*weight.shape) * np.sqrt(2. / (in_dim + out_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Wandb(in_dim, out_dim):\n",
    "    W = np.random.normal(loc=0, scale=0.1, size=(in_dim, out_dim))\n",
    "    b = np.random.normal(loc=0, scale=0.1, size=(1, out_dim))\n",
    "    return Tensor(W, name='weights'), Tensor(b, name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@forks_aware\n",
    "class Linear(Function):\n",
    "    def __init__(self, x: Tensor, W: Tensor, b: Tensor = None):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self):\n",
    "        outputs = np.dot(self.x.data, self.W.data) + self.b.data\n",
    "        return Tensor(outputs, func=self, name=\"linear\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Linear: x: {self.x.shape} W: {self.W.shape} b: {self.b.shape} grad: {grad.shape}')\n",
    "        dW = np.dot(self.x.data.T, grad)\n",
    "        db = grad.sum(axis=0)\n",
    "        grad = np.dot(grad, self.W.data.T)\n",
    "        self.W.backward(dW.reshape(self.W.shape))\n",
    "        self.b.backward(db.reshape(self.b.shape))\n",
    "        self.x.backward(grad.reshape(self.x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LinearLayer(Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.W, self.b = Wandb(in_dim, out_dim)\n",
    "        self.register_parameters([self.W, self.b])\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return Linear(x, self.W, self.b)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@forks_aware\n",
    "class EmbeddingFunction(Function):\n",
    "    def __init__(self, x: Tensor, E: Tensor):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.E = E\n",
    "\n",
    "    def __call__(self):\n",
    "        embeddings = self.E.data[self.x.data.astype('int'), :]\n",
    "        return Tensor(embeddings, func=self, name=\"embedding\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Embedding: x: {self.x.shape} E: {self.E.shape} grad: {grad.shape}')\n",
    "        dE = np.zeros_like(self.E.data)\n",
    "        np.add.at(dE, self.x.data, grad)\n",
    "        self.E.backward(dE.reshape(self.E.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Embedding(Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        self.E = Tensor(np.random.normal(loc=0, scale=0.1, size=(vocab_size, emb_size)), name='E')\n",
    "        self.register_parameters([self.E])\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return EmbeddingFunction(x, self.E)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sigmoid(x):\n",
    "    s = 1.0 / (1.0 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@forks_aware\n",
    "class Sigmoid(Function):\n",
    "    def __init__(self, x: Tensor):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "\n",
    "    def __call__(self):\n",
    "        self.a = sigmoid(self.x.data)\n",
    "        return Tensor(self.a, func=self, name=\"sigmoid\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Sigmoid: x: {self.x.shape} grad: {grad.shape}')\n",
    "        grad = self.a * (1. - self.a) * grad.reshape(self.a.shape)\n",
    "        self.x.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SigmoidFunction(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return Sigmoid(x)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@forks_aware\n",
    "class Tanh(Function):\n",
    "    def __init__(self, x: Tensor):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "\n",
    "    def __call__(self):\n",
    "        self.a = np.tanh(self.x.data)\n",
    "        return Tensor(self.a, func=self, name=\"tanh\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Tanh: x: {self.x.shape} grad: {grad.shape}')\n",
    "        grad = (1. - self.a ** 2) * grad.reshape(self.a.shape)\n",
    "        self.x.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TanhFunction(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return Tanh(x)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@forks_aware\n",
    "class GetHStack(Function):\n",
    "    def __init__(self, x1: Tensor, x2: Tensor):\n",
    "        super().__init__()\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def __call__(self):\n",
    "        stacked = np.hstack((self.x1.data, self.x2.data))\n",
    "        return Tensor(stacked, func=self, name=\"hstack\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'HStack: x1: {self.x1.shape} x2: {self.x2.shape} grad: {grad.shape}')\n",
    "        assert grad.shape[1] == (self.x1.shape[1] + self.x2.shape[1])\n",
    "        self.x1.backward(grad[:, :self.x1.shape[1]])\n",
    "        self.x2.backward(grad[:, self.x1.shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HStack(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor):\n",
    "        return GetHStack(x1, x2)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@forks_aware\n",
    "class GetVStack(Function):\n",
    "    def __init__(self, x1: Tensor, x2: Tensor):\n",
    "        super().__init__()\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def __call__(self):\n",
    "        stacked = np.vstack((self.x1.data, self.x2.data))\n",
    "        return Tensor(stacked, func=self, name=\"vstack\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'VStack: x1: {self.x1.shape} x2: {self.x2.shape} grad: {grad.shape}')\n",
    "        grad = grad.reshape((self.x1.shape[0] + self.x2.shape[0], self.x1.shape[1], -1))\n",
    "        self.x1.backward(grad[:self.x1.shape[0], :])\n",
    "        self.x2.backward(grad[self.x1.shape[0]:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VStack(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor):\n",
    "        return GetVStack(x1, x2)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@forks_aware\n",
    "class GetRow(Function):\n",
    "    def __init__(self, x: Tensor, row_idx: int):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.row_idx = row_idx\n",
    "\n",
    "    def __call__(self):\n",
    "        row = self.x.data[self.row_idx]\n",
    "        return Tensor(row, func=self, name=\"row_\"+str(self.row_idx))\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Row: x: {self.x.shape} grad: {grad.shape}')\n",
    "        dx = np.zeros_like(self.x.data)\n",
    "        dx[self.row_idx] = 1\n",
    "        dx *= grad\n",
    "        self.x.backward(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Row(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor, idx: int):\n",
    "        return GetRow(x, idx)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def softmax_numpy(x):\n",
    "    a = np.amax(x, axis=1)[:, np.newaxis]\n",
    "    ex = np.exp(x - a)\n",
    "    ex_sum = np.sum(ex, axis=1)[:, np.newaxis]\n",
    "    out = ex / ex_sum\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def softmax(x: Tensor):\n",
    "    out = softmax_numpy(x.data)\n",
    "    return Tensor(out, func=x.func, name=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@forks_aware\n",
    "class SoftMax(Function):\n",
    "    def __init__(self, x: Tensor):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "\n",
    "    def __call__(self):\n",
    "        self.a = softmax_numpy(self.x.data)\n",
    "        return Tensor(self.a, func=self, name=\"softmax\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Softmax: x: {self.x.shape} grad: {grad.shape}')\n",
    "        a = self.a.reshape(-1, 1)\n",
    "        grad = np.diagflat(a) - np.dot(a, a.T)\n",
    "        self.x.backward(grad.reshape(self.x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SoftMaxFunction(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return SoftMax(x)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@forks_aware\n",
    "class NLL(Function):\n",
    "    def __init__(self, y_hat: Tensor, y: Tensor, eps: float = 1e-15):\n",
    "        super().__init__()\n",
    "        self.seq_len, self.batch_size = y.shape[0], y.shape[-1]\n",
    "        num_classes = y_hat.shape[-1]\n",
    "        self.y_hat = softmax(y_hat)\n",
    "        self.y = one_hot_encoder(y, num_classes)\n",
    "        self.y = self.y.reshape(-1, num_classes)\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self):\n",
    "        logs = np.log(self.y_hat.data + self.eps)\n",
    "        loss = np.multiply(-self.y.data, logs).sum(axis=1).mean()\n",
    "        return Tensor(loss, func=self, name=\"nll\")\n",
    "\n",
    "    def backward(self):\n",
    "        grad = self.y_hat.data - self.y.data\n",
    "        self.y_hat.backward(grad / float(self.batch_size)/ float(self.seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CrossEntropyLoss(Module):\n",
    "    def __init__(self, eps=1e-15):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        return NLL(output, target, self.eps)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN specific operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MultiplyFunction(Function):\n",
    "    def __init__(self, x1: Tensor, x2: Tensor):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def __call__(self):\n",
    "        return Tensor(self.x1.data * self.x2.data, func=self)\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        grad = grad.reshape(self.x1.shape)\n",
    "        self.x1.backward(self.x2.data*grad)\n",
    "        self.x2.backward(self.x1.data*grad)\n",
    "\n",
    "\n",
    "class Multiply(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor):\n",
    "        return MultiplyFunction(x1, x2)()\n",
    "\n",
    "\n",
    "class SumFunction(Function):\n",
    "    def __init__(self, x1: Tensor, x2: Tensor):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def __call__(self):\n",
    "        return Tensor(self.x1.data+self.x2.data, func=self)\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        grad = grad.reshape(self.x1.shape)\n",
    "        self.x1.backward(grad)\n",
    "        self.x2.backward(grad)\n",
    "\n",
    "\n",
    "class Sum(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor):\n",
    "        return SumFunction(x1, x2)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
